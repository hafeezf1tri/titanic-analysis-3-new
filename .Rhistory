}
} else {
trend <- " (baseline)"
}
cat(sprintf("Run %s: %.4f%s\n",
format(run_info$Timestamp, "%Y-%m-%d %H:%M:%S"),
run_info$ValidationRMSE,
trend))
}
# Overall trend assessment
first_accuracy <- historical_results$ValidationAccuracy[1]
last_accuracy <- tail(historical_results$ValidationAccuracy, 1)
overall_change <- (last_accuracy - first_accuracy) * 100
cat("\nOverall trend from first to most recent run:\n")
if (overall_change > 0) {
cat(sprintf("Validation accuracy IMPROVED by %.2f percentage points\n", overall_change))
} else if (overall_change < 0) {
cat(sprintf("Validation accuracy DEGRADED by %.2f percentage points\n", abs(overall_change)))
} else {
cat("Validation accuracy remained UNCHANGED\n")
}
return(recent_runs)
} else {
cat("\nOnly one run found in history. Need more runs to track performance changes.\n")
return(NULL)
}
} else {
cat("\nNo historical data found. This is the first run.\n")
return(NULL)
}
}
# Main function to run the entire workflow
run_titanic_analysis <- function(file_path = "Titanic_Cleaned.csv") {
cat("=== Starting Titanic Survival Prediction with Decision Tree Regression ===\n\n")
# Preprocess data
titanic_data <- preprocess_data(file_path)
# Split data
data_splits <- split_data(titanic_data)
# Iterate through different complexity parameters to find the best model
cat("\n=== Training Decision Trees with Different Complexity Parameters ===\n")
cp_results <- train_decision_trees(data_splits$train, data_splits$val)
# Find the optimal CP without plotting
cat("\n=== Finding Optimal Complexity Parameter ===\n")
optimal_cp <- find_optimal_cp(cp_results$results)
# Train the final model with the optimal CP value
cat("\n=== Training Final Model with Optimal CP ===\n")
final_model <- rpart(Survived ~ .,
data = data_splits$train,
method = "anova",
control = rpart.control(cp = optimal_cp))
# Print model summary
cat("\nFinal Decision Tree Model Summary:\n")
print(final_model)
# Visualize the decision tree
visualize_tree(final_model)
# Analyze feature importance
feature_importance <- analyze_features(final_model)
# Evaluate the model on all datasets
cat("\n=== Evaluating Model Performance ===\n")
train_eval <- evaluate_model(final_model, data_splits$train, "Training")
val_eval <- evaluate_model(final_model, data_splits$val, "Validation")
test_eval <- evaluate_model(final_model, data_splits$test, "Test")
# Summary of model performance
cat("\n=== Summary of Decision Tree Regression Performance ===\n")
cat("  Training Accuracy:", round(train_eval$accuracy * 100, 2), "%\n")
cat("  Validation Accuracy:", round(val_eval$accuracy * 100, 2), "%\n")
cat("  Test Accuracy:", round(test_eval$accuracy * 100, 2), "%\n")
cat("  Training RMSE:", round(train_eval$rmse, 4), "\n")
cat("  Validation RMSE:", round(val_eval$rmse, 4), "\n")
cat("  Test RMSE:", round(test_eval$rmse, 4), "\n")
# Create a data frame with the run results
results_df <- data.frame(
Timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
RunID = format(Sys.time(), "%Y%m%d%H%M%S"),
ModelType = "Decision Tree Regression",
OptimalCP = optimal_cp,
TrainingAccuracy = train_eval$accuracy,
ValidationAccuracy = val_eval$accuracy,
TestAccuracy = test_eval$accuracy,
TrainingRMSE = train_eval$rmse,
ValidationRMSE = val_eval$rmse,
TestRMSE = test_eval$rmse,
TrainingR2 = train_eval$r_squared,
ValidationR2 = val_eval$r_squared,
TestR2 = test_eval$r_squared,
TrainingF1 = train_eval$f1_score,
ValidationF1 = val_eval$f1_score,
TestF1 = test_eval$f1_score
)
# Define the output file name
results_file <- "titanic_dt_regression_performance.csv"
# Check if the file already exists
if (!file.exists(results_file)) {
# If the file doesn't exist, write the data frame with headers
write.csv(results_df, results_file, row.names = FALSE)
cat("\nCreated new results file:", results_file, "\n")
} else {
# If the file exists, append the new results without headers
write.table(results_df, results_file, append = TRUE, sep = ",",
row.names = FALSE, col.names = FALSE)
cat("\nAppended results to existing file:", results_file, "\n")
}
# Track validation performance over time
cat("\n=== Tracking Validation Performance Over Time ===\n")
performance_trend <- track_validation_performance(results_file)
# Save the model
model_file <- "titanic_decision_tree_reg_model.rds"
saveRDS(final_model, model_file)
cat("\nSaved model to file:", model_file, "\n")
cat("\n=== Analysis Complete ===\n")
return(list(
model = final_model,
results = results_df,
feature_importance = feature_importance,
data_splits = data_splits,
evaluations = list(
train = train_eval,
val = val_eval,
test = test_eval
)
))
}
# Run the full analysis
results <- run_titanic_analysis("Titanic_Cleaned.csv")
# Install required packages if not already installed
if (!require("e1071")) install.packages("e1071")  # For SVM/SVR
if (!require("caret")) install.packages("caret")
if (!require("dplyr")) install.packages("dplyr")
if (!require("ggplot2")) install.packages("ggplot2")
# Load required libraries
library(e1071)  # For SVR
library(caret)
library(dplyr)
library(ggplot2)
# Read the Titanic dataset
titanic_data <- read.csv("Titanic_Cleaned.csv")
# Display basic information about the dataset
cat("Dataset dimensions:", dim(titanic_data), "\n")
cat("\nFirst few rows of the dataset:\n")
print(head(titanic_data))
cat("\nSummary of the dataset:\n")
print(summary(titanic_data))
# Check for the 'No.' column and remove it if it exists
if ("No." %in% colnames(titanic_data)) {
titanic_data <- titanic_data %>% select(-`No.`)
cat("\nRemoved 'No.' column from the dataset.\n")
}
# Convert 'Sex' to a factor variable
titanic_data$Sex <- as.factor(titanic_data$Sex)
# For regression, keep Survived as numeric (0 or 1)
# If Survived is already a factor, convert it to numeric
if (is.factor(titanic_data$Survived)) {
titanic_data$Survived <- as.numeric(as.character(titanic_data$Survived))
}
# Create indices for splitting the data into train (80%), validation (10%), and test (10%)
set.seed(42) # For reproducibility
# Calculate the sample sizes
n <- nrow(titanic_data)
train_size <- floor(0.8 * n)
val_size <- floor(0.1 * n)
test_size <- n - train_size - val_size
# Create indices for the different sets
indices <- sample(1:n, n)
train_indices <- indices[1:train_size]
val_indices <- indices[(train_size + 1):(train_size + val_size)]
test_indices <- indices[(train_size + val_size + 1):n]
# Split the data
train_data <- titanic_data[train_indices, ]
val_data <- titanic_data[val_indices, ]
test_data <- titanic_data[test_indices, ]
# Verify the split sizes
cat("\nSplit sizes:\n")
cat("Training set:", nrow(train_data), "observations (", round(nrow(train_data)/n * 100, 2), "%)\n")
cat("Validation set:", nrow(val_data), "observations (", round(nrow(val_data)/n * 100, 2), "%)\n")
cat("Test set:", nrow(test_data), "observations (", round(nrow(test_data)/n * 100, 2), "%)\n")
# Tune SVR hyperparameters using cross-validation on the training set
cat("\nTuning SVR hyperparameters using cross-validation...\n")
tune_results <- tune.svm(Survived ~ .,
data = train_data,
type = "eps-regression",
kernel = "radial",
gamma = c(0.01, 0.05, 0.1, 0.5),
cost = c(1, 5, 10, 50),
epsilon = c(0.01, 0.05, 0.1, 0.5),
tunecontrol = tune.control(cross = 5))  # 5-fold cross-validation
# Print tuning results
cat("\nHyperparameter tuning results:\n")
print(tune_results)
# Get the best model parameters
best_params <- tune_results$best.parameters
cat("\nBest parameters:\n")
print(best_params)
# Train the final SVR model with the best parameters
svr_model <- svm(Survived ~ .,
data = train_data,
type = "eps-regression",
kernel = "radial",
gamma = best_params$gamma,
cost = best_params$cost,
epsilon = best_params$epsilon)
# Print model summary
cat("\nSupport Vector Regression Model Summary:\n")
print(svr_model)
# Function to evaluate regression model performance
evaluate_reg_model <- function(model, data, dataset_name) {
# Get actual target values
actual <- data$Survived
# Make predictions
predictions <- predict(model, data)
# Convert predictions to binary classification (for accuracy calculation)
binary_predictions <- ifelse(predictions >= 0.5, 1, 0)
# Calculate metrics
mse <- mean((predictions - actual)^2)
rmse <- sqrt(mse)
mae <- mean(abs(predictions - actual))
r_squared <- 1 - sum((actual - predictions)^2) / sum((actual - mean(actual))^2)
accuracy <- mean(binary_predictions == actual)
# Calculate sensitivity, specificity, precision
true_pos <- sum(binary_predictions == 1 & actual == 1)
true_neg <- sum(binary_predictions == 0 & actual == 0)
false_pos <- sum(binary_predictions == 1 & actual == 0)
false_neg <- sum(binary_predictions == 0 & actual == 1)
sensitivity <- true_pos / (true_pos + false_neg)
specificity <- true_neg / (true_neg + false_pos)
precision <- true_pos / (true_pos + false_pos)
f1_score <- 2 * precision * sensitivity / (precision + sensitivity)
# Print results
cat("\n=== ", dataset_name, " Set Regression Evaluation ===\n")
cat("MSE:", round(mse, 4), "\n")
cat("RMSE:", round(rmse, 4), "\n")
cat("MAE:", round(mae, 4), "\n")
cat("R-squared:", round(r_squared, 4), "\n")
cat("Accuracy (using 0.5 threshold):", round(accuracy * 100, 2), "%\n")
cat("Sensitivity/Recall:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
cat("F1 Score:", round(f1_score, 4), "\n")
# Confusion matrix
conf_matrix <- table(Actual = actual, Predicted = binary_predictions)
cat("\nConfusion Matrix:\n")
print(conf_matrix)
return(list(
mse = mse,
rmse = rmse,
mae = mae,
r_squared = r_squared,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
precision = precision,
f1_score = f1_score,
conf_matrix = conf_matrix
))
}
# Evaluate the model on all datasets
train_eval <- evaluate_reg_model(svr_model, train_data, "Training")
val_eval <- evaluate_reg_model(svr_model, val_data, "Validation")
test_eval <- evaluate_reg_model(svr_model, test_data, "Test")
# Summary of regression model performance
cat("\n=== Summary of Regression Model Performance ===\n")
cat("  Training Accuracy:", round(train_eval$accuracy * 100, 2), "%\n")
cat("  Validation Accuracy:", round(val_eval$accuracy * 100, 2), "%\n")
cat("  Test Accuracy:", round(test_eval$accuracy * 100, 2), "%\n")
cat("  Training RMSE:", round(train_eval$rmse, 4), "\n")
cat("  Validation RMSE:", round(val_eval$rmse, 4), "\n")
cat("  Test RMSE:", round(test_eval$rmse, 4), "\n")
# Create visualization of predictions vs actual values
plot_data <- data.frame(
Actual = test_data$Survived,
Predicted = predict(svr_model, test_data)
)
ggplot(plot_data, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "SVR: Actual vs Predicted Survival",
x = "Actual Survival Value",
y = "Predicted Survival Value") +
theme_minimal()
# Create a data frame with the run results
results_df <- data.frame(
Timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
RunID = format(Sys.time(), "%Y%m%d%H%M%S"),
ModelType = "SVR",
TrainingAccuracy = train_eval$accuracy,
ValidationAccuracy = val_eval$accuracy,
TestAccuracy = test_eval$accuracy,
TrainingRMSE = train_eval$rmse,
ValidationRMSE = val_eval$rmse,
TestRMSE = test_eval$rmse,
TrainingR2 = train_eval$r_squared,
ValidationR2 = val_eval$r_squared,
TestR2 = test_eval$r_squared,
TrainingF1 = train_eval$f1_score,
ValidationF1 = val_eval$f1_score,
TestF1 = test_eval$f1_score,
TrainSize = nrow(train_data),
ValidationSize = nrow(val_data),
TestSize = nrow(test_data)
)
# Define the model type and output file name
model_type <- "svr"
results_file <- paste0("titanic_", model_type, "_performance.csv")
# Check if the file already exists
if (!file.exists(results_file)) {
# If the file doesn't exist, write the data frame with headers
write.csv(results_df, results_file, row.names = FALSE)
cat("\nCreated new results file:", results_file, "\n")
} else {
# If the file exists, append the new results without headers
write.table(results_df, results_file, append = TRUE, sep = ",",
row.names = FALSE, col.names = FALSE)
cat("\nAppended results to existing file:", results_file, "\n")
}
# Print the data that was saved to the CSV
cat("\nSaved the following data to", results_file, ":\n")
print(results_df)
# Save the model
saveRDS(svr_model, "titanic_svr_model.rds")
# Install required packages if not already installed
if (!require("e1071")) install.packages("e1071")  # For SVM/SVR
if (!require("caret")) install.packages("caret")
if (!require("dplyr")) install.packages("dplyr")
if (!require("ggplot2")) install.packages("ggplot2")
# Load required libraries
library(e1071)  # For SVR
library(caret)
library(dplyr)
library(ggplot2)
# Read the Titanic dataset
titanic_data <- read.csv("Titanic_Cleaned.csv")
# Display basic information about the dataset
cat("Dataset dimensions:", dim(titanic_data), "\n")
cat("\nFirst few rows of the dataset:\n")
print(head(titanic_data))
cat("\nSummary of the dataset:\n")
print(summary(titanic_data))
# Check for the 'No.' column and remove it if it exists
if ("No." %in% colnames(titanic_data)) {
titanic_data <- titanic_data %>% select(-`No.`)
cat("\nRemoved 'No.' column from the dataset.\n")
}
# Convert 'Sex' to a factor variable
titanic_data$Sex <- as.factor(titanic_data$Sex)
# For regression, keep Survived as numeric (0 or 1)
# If Survived is already a factor, convert it to numeric
if (is.factor(titanic_data$Survived)) {
titanic_data$Survived <- as.numeric(as.character(titanic_data$Survived))
}
# Create indices for splitting the data into train (80%), validation (10%), and test (10%)
set.seed(42) # For reproducibility
# Calculate the sample sizes
n <- nrow(titanic_data)
train_size <- floor(0.8 * n)
val_size <- floor(0.1 * n)
test_size <- n - train_size - val_size
# Create indices for the different sets
indices <- sample(1:n, n)
train_indices <- indices[1:train_size]
val_indices <- indices[(train_size + 1):(train_size + val_size)]
test_indices <- indices[(train_size + val_size + 1):n]
# Split the data
train_data <- titanic_data[train_indices, ]
val_data <- titanic_data[val_indices, ]
test_data <- titanic_data[test_indices, ]
# Verify the split sizes
cat("\nSplit sizes:\n")
cat("Training set:", nrow(train_data), "observations (", round(nrow(train_data)/n * 100, 2), "%)\n")
cat("Validation set:", nrow(val_data), "observations (", round(nrow(val_data)/n * 100, 2), "%)\n")
cat("Test set:", nrow(test_data), "observations (", round(nrow(test_data)/n * 100, 2), "%)\n")
# Tune SVR hyperparameters using cross-validation on the training set
cat("\nTuning SVR hyperparameters using cross-validation...\n")
tune_results <- tune.svm(Survived ~ .,
data = train_data,
type = "eps-regression",
kernel = "radial",
gamma = c(0.01, 0.05, 0.1, 0.5),
cost = c(1, 5, 10, 50),
epsilon = c(0.01, 0.05, 0.1, 0.5),
tunecontrol = tune.control(cross = 5))  # 5-fold cross-validation
# Print tuning results
cat("\nHyperparameter tuning results:\n")
print(tune_results)
# Get the best model parameters
best_params <- tune_results$best.parameters
cat("\nBest parameters:\n")
print(best_params)
# Train the final SVR model with the best parameters
svr_model <- svm(Survived ~ .,
data = train_data,
type = "eps-regression",
kernel = "radial",
gamma = best_params$gamma,
cost = best_params$cost,
epsilon = best_params$epsilon)
# Print model summary
cat("\nSupport Vector Regression Model Summary:\n")
print(svr_model)
# Function to evaluate regression model performance
evaluate_reg_model <- function(model, data, dataset_name) {
# Get actual target values
actual <- data$Survived
# Make predictions
predictions <- predict(model, data)
# Convert predictions to binary classification (for accuracy calculation)
binary_predictions <- ifelse(predictions >= 0.5, 1, 0)
# Calculate metrics
mse <- mean((predictions - actual)^2)
rmse <- sqrt(mse)
mae <- mean(abs(predictions - actual))
r_squared <- 1 - sum((actual - predictions)^2) / sum((actual - mean(actual))^2)
accuracy <- mean(binary_predictions == actual)
# Calculate sensitivity, specificity, precision
true_pos <- sum(binary_predictions == 1 & actual == 1)
true_neg <- sum(binary_predictions == 0 & actual == 0)
false_pos <- sum(binary_predictions == 1 & actual == 0)
false_neg <- sum(binary_predictions == 0 & actual == 1)
sensitivity <- true_pos / (true_pos + false_neg)
specificity <- true_neg / (true_neg + false_pos)
precision <- true_pos / (true_pos + false_pos)
f1_score <- 2 * precision * sensitivity / (precision + sensitivity)
# Print results
cat("\n=== ", dataset_name, " Set Regression Evaluation ===\n")
cat("MSE:", round(mse, 4), "\n")
cat("RMSE:", round(rmse, 4), "\n")
cat("MAE:", round(mae, 4), "\n")
cat("R-squared:", round(r_squared, 4), "\n")
cat("Accuracy (using 0.5 threshold):", round(accuracy * 100, 2), "%\n")
cat("Sensitivity/Recall:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
cat("F1 Score:", round(f1_score, 4), "\n")
# Confusion matrix
conf_matrix <- table(Actual = actual, Predicted = binary_predictions)
cat("\nConfusion Matrix:\n")
print(conf_matrix)
return(list(
mse = mse,
rmse = rmse,
mae = mae,
r_squared = r_squared,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
precision = precision,
f1_score = f1_score,
conf_matrix = conf_matrix
))
}
# Evaluate the model on all datasets
train_eval <- evaluate_reg_model(svr_model, train_data, "Training")
val_eval <- evaluate_reg_model(svr_model, val_data, "Validation")
test_eval <- evaluate_reg_model(svr_model, test_data, "Test")
# Summary of regression model performance
cat("\n=== Summary of Regression Model Performance ===\n")
cat("  Training Accuracy:", round(train_eval$accuracy * 100, 2), "%\n")
cat("  Validation Accuracy:", round(val_eval$accuracy * 100, 2), "%\n")
cat("  Test Accuracy:", round(test_eval$accuracy * 100, 2), "%\n")
cat("  Training RMSE:", round(train_eval$rmse, 4), "\n")
cat("  Validation RMSE:", round(val_eval$rmse, 4), "\n")
cat("  Test RMSE:", round(test_eval$rmse, 4), "\n")
# Create visualization of predictions vs actual values
plot_data <- data.frame(
Actual = test_data$Survived,
Predicted = predict(svr_model, test_data)
)
ggplot(plot_data, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "SVR: Actual vs Predicted Survival",
x = "Actual Survival Value",
y = "Predicted Survival Value") +
theme_minimal()
# Create a data frame with the run results
results_df <- data.frame(
Timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
RunID = format(Sys.time(), "%Y%m%d%H%M%S"),
ModelType = "SVR",
TrainingAccuracy = train_eval$accuracy,
ValidationAccuracy = val_eval$accuracy,
TestAccuracy = test_eval$accuracy,
TrainingRMSE = train_eval$rmse,
ValidationRMSE = val_eval$rmse,
TestRMSE = test_eval$rmse,
TrainingR2 = train_eval$r_squared,
ValidationR2 = val_eval$r_squared,
TestR2 = test_eval$r_squared,
TrainingF1 = train_eval$f1_score,
ValidationF1 = val_eval$f1_score,
TestF1 = test_eval$f1_score,
TrainSize = nrow(train_data),
ValidationSize = nrow(val_data),
TestSize = nrow(test_data)
)
# Define the model type and output file name
model_type <- "svr"
results_file <- paste0("titanic_", model_type, "_performance.csv")
# Check if the file already exists
if (!file.exists(results_file)) {
# If the file doesn't exist, write the data frame with headers
write.csv(results_df, results_file, row.names = FALSE)
cat("\nCreated new results file:", results_file, "\n")
} else {
# If the file exists, append the new results without headers
write.table(results_df, results_file, append = TRUE, sep = ",",
row.names = FALSE, col.names = FALSE)
cat("\nAppended results to existing file:", results_file, "\n")
}
# Print the data that was saved to the CSV
cat("\nSaved the following data to", results_file, ":\n")
print(results_df)
# Save the model
saveRDS(svr_model, "titanic_svr_model.rds")
